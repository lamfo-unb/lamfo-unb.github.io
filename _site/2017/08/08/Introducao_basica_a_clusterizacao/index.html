<h1 id="introdução-básica-à-clusterização">Introdução básica à Clusterização</h1>

<h2 id="o-que-é-clusterização">O que é Clusterização?</h2>

<p>Clusterização é o agrupamento automático de instâncias similares, uma <strong>classificação não-supervisionada</strong> dos dados (<a href="https://lamfo-unb.github.io/2017/07/27/tres-tipos-am/" title="Os Três Tipos de Aprendizado de Máquina">esse termo não é familiar? Veja nosso post sobre os três tipos de aprendizado de máquina!</a>). Ou seja, um algorítmo que clusteriza dados classifica eles em conjuntos de dados que ‘se assemelham’ de alguma forma - independentemente de classes predefinidas. <strong>Os grupos gerados por essa classificação são chamados <em>clusters</em></strong>.</p>

<p>Uma forma de clusterização seria, por exemplo, a partir de dados de animais em um zoológico aproximar animais por suas características. Ou seja, a partir dos dados como ‘quantidade de pernas’, ‘quantidade de dentes’, ‘põe ovo’, ‘tem pêlos’ e vários outros, procuramos animais que estão mais próximos. Poderíamos assim clusterizar os dados, separar animais em mamíferos, aves ou répteis mas sem “contar” ao algorítmo sobre estas classificações. Apenas comparando a distância entre dados o algorítmo mostraria que um tigre “está mais próximo” de um leão do que de uma garça.</p>

<p>As imagens a seguir ilustram uma clusterização bem simples de dados com apenas duas dimensões (duas “características”):</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Dados</th>
      <th style="text-align: center">Dados agrupados em clusters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/img/clustering/01-clustering.png" alt="" /></td>
      <td style="text-align: center"><img src="/img/clustering/02-clustering.png" alt="" /></td>
    </tr>
  </tbody>
</table>

<p>Muitas vezes, a similaridade entre os dados é encontrada por métricas de distância. Um dos algorítmos mais básicos para Clusterização chama-se <strong>K-Means</strong>.</p>

<h2 id="k-means">K-Means</h2>

<p>O algorítmo se chama assim pois <strong>encontra k <em>clusters</em> diferentes</strong> no conjunto de dados. O <strong>centro de cada <em>cluster</em> será chamado centróide</strong> e terá a média dos valores neste cluster.</p>

<p>A tarefa do algorítmo é encontrar o centróide mais próximo (por meio de alguma métrica de distância) e atribuir o ponto encontrado a esse cluster. Após este passo, os centróides são atualizados sempre tomando o valor médio de todos os pontos naquele cluster. Para este método são necessários valores numéricos para o cálculo da distância, os valores nominais então podem ser mapeados em valores binários para o mesmo cálculo. Em caso de sucesso, os <strong>dados são separados organicamente</strong> podendo assim ser rotulados e <strong>centróides viram referência</strong> para classificar novos dados.</p>

<p>Para o exemplo utilizaremos o <a href="https://archive.ics.uci.edu/ml/datasets/iris"><em>Iris Data Set</em></a> do <em>UCI Machine Learning Repository</em>. Este é um dos conjuntos de dados mais conhecidos e utilizados para exemplos simples de reconhecimento de padrões. O conjunto de dados contém 3 classes de 50 instâncias cada, onde cada classe se refere a um tipo de planta Iris.</p>

<h3 id="passo-a-passo-do-algorítmo">Passo a passo do algorítmo</h3>

<p>O Κ-means aprimora de forma iterativa seus resultados até alcançar um resultado final. O algoritmo recebe o número de clusters Κ e o conjunto de dados sob análise. Em seguida são estabelecidas estimativas iniciais para os K centróides, que podem ser gerados aleatoriamente ou selecionados aleatoriamente dentro conjunto de dados. O algoritmo faz a iteração entre dois passos:</p>

<ul>
  <li>
    <p><strong>Associação de cada instância a um centróide</strong> - cada centróide define um cluster, então cada instância será associada a seu cluster mais semelhante (centróide mais próximo). A distância será calculada por alguma métrica de distância, em geral utiliza-se a distância euclidiana entre as duas instâncias;</p>
  </li>
  <li>
    <p><strong>Atualização dos centróides</strong> - centróides dos clusters são recalculados, a partir da média entre todas as instâncias associadas àquele cluster.</p>
  </li>
</ul>

<h3 id="na-prática-com-python">Na prática com Python</h3>

<p>Para que possamos testar o algorítmo utilizaremos a <strong>linguagem Python</strong> e algumas de suas bibliotecas</p>

<ul>
  <li>Se você ainda não tem Python em sua máquina, dê uma olhada no nosso post <a href="https://lamfo-unb.github.io/2017/06/10/Instalando-Python/">Instalando Python para Aprendizado de Máquina</a>;</li>
  <li>Crie um diretório de trabalho em sua máquina, uma pasta que conterá seu programa e os dados utilizados. Decidi chamar meu diretório de <em>‘learn-clustering’</em>;</li>
  <li>Faça download do <a href="http://archive.ics.uci.edu/ml/machine-learning-databases/iris/"><em>iris.data</em></a> no seu novo diretório;</li>
  <li>Crie um novo arquivo python onde escreveremos nosso programa no mesmo diretório principal ou um Jupyter Notebook. Os trechos de código a seguir devem ser codificados dentro do novo arquivo criado.</li>
</ul>

<h4 id="1-importando-as-pacotes">1. Importando as pacotes</h4>

<p>Usaremos:</p>
<ol>
  <li>NumPy (pacote básico para computação científica e matemática, com diversas funções e operações sofisticadas)</li>
  <li>Pandas (pacote para manipulação e estruturação de dados)</li>
  <li>Matplotlib (pacote para plotagens gráficas em 2D)</li>
  <li>sklearn (pacote de Machine Learning contendo a ferramento do algorítmo KMeans pronta)</li>
</ol>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span> <span class="c"># 1</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span> <span class="c"># 2</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span> <span class="c"># 3</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span> <span class="c"># 4</span>
</code></pre>
</div>

<h4 id="2-lendo-o-dataset">2. Lendo o Dataset</h4>

<p>A leitura dos dados é feita a partir da biblioteca Pandas e os dados estão organizados no formato csv (<em>comma-separated values</em>) apesar da extensão <em>‘.data’</em>. Chamaremos o dataset completo de <em>dataset</em></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s">'sepal length'</span><span class="p">,</span> <span class="s">'sepal width'</span><span class="p">,</span> <span class="s">'petal length'</span><span class="p">,</span> <span class="s">'class'</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"./iris.data"</span><span class="p">,</span> <span class="n">encoding</span> <span class="o">=</span> <span class="s">"ISO-8859-1"</span><span class="p">,</span> <span class="n">decimal</span><span class="o">=</span><span class="s">","</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">headers</span><span class="p">)</span>
</code></pre>
</div>

<p>Os dados não têm indicação dos nomes das colunas, utilizaremos o <em>array ‘headers’</em> para nomeá-las corretamente.
A variável <em>‘dataset’</em> recebe os dados lidos, <em>‘header=None’</em> indica que os dados no têm cabeçalho e <em>‘names=headers’</em> faz com que os cabeçalhos deste dataset sejam os definidos na variável <em>‘headers’</em></p>

<p>Trecho do Dataset:</p>

<p>sepal length | sepal width | petal length | class
– | – | – | –
3.5 | 1.4 | 0.2 | Iris-setosa
3.0 | 1.4 | 0.2 | Iris-setosa
3.2 | 1.3 | 0.2 | Iris-setosa
…
2.8 | 4.7 | 1.2 | Iris-versicolor
2.9 | 4.3 | 1.3 | Iris-versicolor
3.0 | 4.4 | 1.4 | Iris-versicolor
…
3.0 | 5.2 | 2.0 | Iris-virginica
3.4 | 5.4 | 2.3 | Iris-virginica
3.0 | 5.1 | 1.8 | Iris-virginica</p>

<p>Podemos perceber que cada coluna é referente uma característica daquele espécime de planta. Ou seja, cada caso específico de flor produz uma instância diferente, que têm dados próprios.</p>

<h4 id="3-pré-processando-os-dados">3. Pré-processando os dados</h4>

<p>Para garantirmos dados numéricos, vamos garantir que as colunas sejam do tipo float:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span>  <span class="n">dataset</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]:</span>
    <span class="n">dataset</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
</code></pre>
</div>

<p>Podemos confirmar os tipos de cada coluna do dataset com o ‘dtypes’</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">dataset</span><span class="o">.</span><span class="n">dtypes</span>

      <span class="n">sepal</span> <span class="n">length</span>    <span class="n">float64</span>
      <span class="n">sepal</span> <span class="n">width</span>     <span class="n">float64</span>
      <span class="n">petal</span> <span class="n">length</span>    <span class="n">float64</span>
      <span class="k">class</span>            <span class="nc">object</span>
      <span class="n">dtype</span><span class="p">:</span> <span class="nb">object</span>
</code></pre>
</div>

<p>Agora deve ser realizada a divisão dos dados entre variáveis dependentes ( X ) e independente ( y ). Uma definição de variáveis dependentes e independentes pode ser encontrada no nosso post <a href="https://lamfo-unb.github.io/2017/07/27/tres-tipos-am/">Os Três Tipos de Aprendizado de Máquina</a></p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span>
</code></pre>
</div>

<p>Agora vamos aplicar o kmeans no conjunto de variáveis dependentes - ou seja, não estamos ‘contando’ ao algorítmo quais são as classes de cada instância de flor, estamos apenas apresentando os dados que cada instância tem. Definimos o número de clusters - k - como 3, uma situação ideal. Existem técnicas para encontrar o melhor k que serão abordadadas em um próximo post.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_clustered</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre>
</div>
<p>Existem várias formas de checar a clusterização realizada, uma bem simples é criando uma tabela de resultados contendo a coluna das classes esperadas e a coluna dos clusters criados:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[[</span><span class="s">'class'</span><span class="p">]]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">results</span><span class="p">[</span><span class="s">'clusterNumber'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_clustered</span>
<span class="n">results</span>
</code></pre>
</div>

<p>Tabela <em>‘results’</em></p>

<p>class	| clusterNumber
– | –
Iris-setosa |	0
Iris-setosa |	0
Iris-setosa |	0
…
Iris-versicolor |	1
Iris-versicolor |	1
Iris-versicolor |	1
…
Iris-virginica |	2
Iris-virginica |	2
Iris-virginica |	2</p>

<p>Neste pequeno dataset é simples visualizar que atingimos a clusterização esperada. Podemos agora testar uma abordagem mais gráfica. Primeiramente vamos definir cores para os diferentes tipos de clusters e ‘pintar’ pontos em clusteres diferentes de cores diferentes:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">LABEL_COLOR_MAP</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span> <span class="p">:</span> <span class="s">'red'</span><span class="p">,</span> <span class="mi">1</span> <span class="p">:</span> <span class="s">'blue'</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s">'green'</span><span class="p">}</span>
<span class="n">label_color</span> <span class="o">=</span> <span class="p">[</span><span class="n">LABEL_COLOR_MAP</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">X_clustered</span><span class="p">]</span>
</code></pre>
</div>

<p>Agora podemos plotar um gráfico que compare duas características em duas dimensões. A lógica a seguir é apenas para que possamos plotar dinamicamente mudando apenas os valores de <em>‘c1’</em> e <em>‘c2’</em> que serão características diferentes a serem comparadas e dispostas nos eixos do gráfico.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">c1</span> <span class="o">=</span> <span class="mi">0</span> <span class="c"># valor do índice da coluna, pode ser 0, 1 ou 2</span>
<span class="n">c2</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s">'sepal length'</span><span class="p">,</span> <span class="s">'sepal width'</span><span class="p">,</span> <span class="s">'petal length'</span><span class="p">]</span>
<span class="n">c1label</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">c1</span><span class="p">]</span>
<span class="n">c2label</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">c12</span><span class="p">]</span>
<span class="n">title</span> <span class="o">=</span> <span class="n">xlabel</span> <span class="o">+</span> <span class="s">' x '</span> <span class="o">+</span> <span class="n">ylabel</span>
</code></pre>
</div>

<p>Com as características escolhidas, plotamos o gráfico com Matplotlib:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">c1</span><span class="p">],</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">c2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">label_color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">c1label</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">c2label</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">title</span> <span class="o">+</span> <span class="s">'.jpg'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>
</div>

<table>
  <thead>
    <tr>
      <th style="text-align: center">sepal length X sepal width</th>
      <th style="text-align: center">sepal length X petal length</th>
      <th style="text-align: center">sepal width X petal length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/img/clustering/03-clustering.jpg" alt="" /></td>
      <td style="text-align: center"><img src="/img/clustering/04-clustering.jpg" alt="" /></td>
      <td style="text-align: center"><img src="/img/clustering/05-clustering.jpg" alt="" /></td>
    </tr>
  </tbody>
</table>

<p>Neste exemplo fica clara a divisão entre classes diferentes, assim podemos compreender como a clusterização funciona e que existem casos específicos em que a técnica será muito útil. Aplicações possíveis da clusterização incluem:</p>

<ul>
  <li>Marketing: Pesquisa e segmentação de mercado para determinar potenciais grupos homogêneos de consumidores para melhor definir a disposição de produtos que uma estratégia corporativa;</li>
  <li>Análise de redes sociais: Dentro de um grande grupo de pesoas, reconhecer comunidades que compartilhem de alguma preferência ou opinião;</li>
  <li>Reconhecimento de imagens: Clusterização pode ser aplicado para se reconhecer uma pessoa ou um objeto numa foto; os <em>clusters</em> seriam as regiões da imagem que contenham rostos, paisagem, vestimentas, etc.;</li>
  <li>Detecção de anomalias: A análise de clusters pode ser adaptada para a detecção de <em>outliers</em> que destoem da maioria dos outros elementos baseada em alguma métrica de similaridade;</li>
  <li>Finanças: clusterização pode ser utilizado para agrupar empresas por similaridade, o que pode ser bem útil para a construção de portfólios, cuja teoria clássica preconiza que o risco do investidor pode ser diversificado quando se aplica em empresas pouco correlacionadas. A clusterização ainda pode ser usada para a identificação de períodos de alta e baixa volatilidade, o que por sua vez pode subsidiar a construção de estratégias de <em>trading</em> e de gestão do risco.</li>
</ul>

<p>Nem todo resultado terão dados ideais como este conjunto e assim como qualquer algorítmo ele funcionará para uma finalidade específica. Por isso, como sempre, conhecer bem o problema, pré-processar os dados corretamente e aplicar técnicas diferentes certamente trarão resultados melhores.</p>
